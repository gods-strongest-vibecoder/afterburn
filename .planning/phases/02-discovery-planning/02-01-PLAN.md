---
phase: 02-discovery-planning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/discovery.ts
  - src/discovery/crawler.ts
  - src/discovery/index.ts
autonomous: true

must_haves:
  truths:
    - "Crawler visits every reachable same-hostname page starting from a seed URL"
    - "Already-visited URLs are not re-crawled (deduplication works)"
    - "Crawler warns user after 50 pages but continues crawling"
    - "Crawler respects concurrency limit (max 3 simultaneous pages)"
  artifacts:
    - path: "src/types/discovery.ts"
      provides: "All Phase 2 type definitions"
      contains: "PageData"
      exports: ["PageData", "InteractiveElement", "FormField", "LinkInfo", "SitemapNode", "CrawlResult", "DiscoveryArtifact"]
    - path: "src/discovery/crawler.ts"
      provides: "Recursive web crawler with URL queue and deduplication"
      exports: ["SiteCrawler"]
      min_lines: 80
    - path: "src/discovery/index.ts"
      provides: "Barrel export for discovery module"
      exports: ["SiteCrawler"]
  key_links:
    - from: "src/discovery/crawler.ts"
      to: "src/browser/browser-manager.ts"
      via: "BrowserManager.newPage() for each URL"
      pattern: "browserManager\\.newPage"
    - from: "src/discovery/crawler.ts"
      to: "src/types/discovery.ts"
      via: "Uses PageData and CrawlResult types"
      pattern: "import.*discovery"
---

<objective>
Define all Phase 2 type definitions and build the recursive web crawler engine that visits every reachable page on a site.

Purpose: The crawler is the foundation for all Phase 2 work. Every other module (element mapper, SPA detector, link validator, sitemap builder) operates on pages the crawler discovers. Types must be defined first so all plans in this phase share a contract.

Output: `src/types/discovery.ts` with all Phase 2 types, `src/discovery/crawler.ts` with a working `SiteCrawler` class that recursively crawls same-hostname pages using the existing BrowserManager.
</objective>

<execution_context>
@C:\Users\Shiv_\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Shiv_\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-discovery-planning/02-CONTEXT.md
@.planning/phases/02-discovery-planning/02-RESEARCH.md
@src/types/artifacts.ts
@src/browser/browser-manager.ts
@src/browser/stealth-browser.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define Phase 2 type definitions</name>
  <files>src/types/discovery.ts</files>
  <action>
Create `src/types/discovery.ts` with all type definitions needed across Phase 2. These types form the contract between crawler, element mapper, SPA detector, link validator, sitemap builder, and workflow planner.

Define these types:

```typescript
// FormField — individual form input/textarea/select
interface FormField {
  type: string;          // input type (text, email, password, etc.)
  name: string;          // field name attribute
  label: string;         // associated label text or aria-label
  required: boolean;
  placeholder: string;
}

// InteractiveElement — any clickable/interactive element on a page
interface InteractiveElement {
  type: 'button' | 'link' | 'input' | 'select' | 'menu' | 'tab' | 'modal-trigger';
  selector: string;      // Playwright selector for this element
  text: string;          // visible text content
  visible: boolean;      // visible on initial load?
  attributes: Record<string, string>;  // relevant attributes (href, type, role, etc.)
}

// FormInfo — a complete form with all its fields
interface FormInfo {
  action: string;        // form action attribute
  method: string;        // GET or POST
  selector: string;      // Playwright selector for the form
  fields: FormField[];
}

// LinkInfo — a link found on a page
interface LinkInfo {
  href: string;          // resolved absolute URL
  text: string;          // link text
  isInternal: boolean;   // same hostname?
  statusCode?: number;   // populated by link validator
}

// PageData — everything discovered about a single page
interface PageData {
  url: string;
  title: string;
  forms: FormInfo[];
  buttons: InteractiveElement[];
  links: LinkInfo[];
  menus: InteractiveElement[];
  otherInteractive: InteractiveElement[];  // tabs, modals, dropdowns
  screenshotRef?: ScreenshotRef;           // from Phase 1 types
  spaFramework?: SPAFramework;
  crawledAt: string;     // ISO 8601
}

// SPAFramework — detected SPA framework info
interface SPAFramework {
  framework: 'react' | 'vue' | 'angular' | 'next' | 'svelte' | 'nuxt' | 'none';
  version?: string;
  router?: string;
}

// SitemapNode — hierarchical tree node for sitemap
interface SitemapNode {
  url: string;
  title: string;
  path: string;          // URL path component (e.g. /dashboard/settings)
  children: SitemapNode[];
  pageData: PageData;
  depth: number;         // depth from root (0 = homepage)
}

// BrokenLink — a link that returned non-2xx status
interface BrokenLink {
  url: string;
  sourceUrl: string;     // page where link was found
  statusCode: number;    // 0 = network error
  statusText: string;
}

// WorkflowStep — single step in a workflow plan
interface WorkflowStep {
  action: 'navigate' | 'click' | 'fill' | 'select' | 'wait' | 'expect';
  selector: string;
  value?: string;
  expectedResult: string;
  confidence: number;    // 0-1
}

// WorkflowPlan — AI-generated test plan
interface WorkflowPlan {
  workflowName: string;
  description: string;
  steps: WorkflowStep[];
  priority: 'critical' | 'important' | 'nice-to-have';
  estimatedDuration: number;  // seconds
  source: 'auto-discovered' | 'user-hint';
}

// CrawlResult — output of the crawler
interface CrawlResult {
  pages: PageData[];
  brokenLinks: BrokenLink[];
  totalPagesDiscovered: number;
  totalLinksChecked: number;
  crawlDuration: number;      // milliseconds
  spaDetected: SPAFramework;
}

// DiscoveryArtifact — complete Phase 2 output artifact (extends ArtifactMetadata)
interface DiscoveryArtifact extends ArtifactMetadata {
  targetUrl: string;
  sitemap: SitemapNode;
  crawlResult: CrawlResult;
  workflowPlans: WorkflowPlan[];
  userHints: string[];         // from --flows flag
}
```

Import `ArtifactMetadata` and `ScreenshotRef` from `../types/artifacts.js`. Export all types.

Add a 1-2 line header comment explaining the file's purpose.
  </action>
  <verify>Run `npx tsc --noEmit` — no type errors.</verify>
  <done>All Phase 2 types defined, exported, and type-check passes.</done>
</task>

<task type="auto">
  <name>Task 2: Build recursive web crawler with URL queue</name>
  <files>src/discovery/crawler.ts, src/discovery/index.ts</files>
  <action>
Create `src/discovery/crawler.ts` with a `SiteCrawler` class that recursively crawls all same-hostname pages.

**Architecture decision:** Build crawler on top of existing BrowserManager rather than using Crawlee. Reason: Our stealth setup uses playwright-extra which conflicts with Crawlee's browser lifecycle management. The URL queue + dedup is simple enough to implement manually, and we get guaranteed stealth compatibility.

The SiteCrawler class:

```typescript
class SiteCrawler {
  private browserManager: BrowserManager;
  private visited: Set<string>;         // normalized URLs already visited
  private queue: string[];              // URLs to visit
  private pages: PageData[];            // collected page data
  private hostname: string;             // seed URL hostname for filtering
  private maxConcurrency: number;       // default 3
  private onPageCrawled?: (url: string, count: number) => void;  // progress callback

  constructor(browserManager: BrowserManager, options?: CrawlerOptions)
  async crawl(seedUrl: string): Promise<CrawlResult>
}
```

**CrawlerOptions interface** (define in this file or in discovery.ts):
```typescript
interface CrawlerOptions {
  maxConcurrency?: number;    // default: 3
  maxPages?: number;          // default: 0 (unlimited, warn at 50)
  excludePatterns?: string[]; // glob patterns to skip (e.g., '*.pdf')
  onPageCrawled?: (url: string, count: number) => void;
}
```

**crawl() method implementation:**

1. Parse seed URL to extract hostname
2. Add seed URL to queue
3. Process queue in batches of maxConcurrency:
   - For each URL in batch: open page via BrowserManager.newPage(url), extract page title, find all `<a>` links on page
   - For each found link: resolve relative URLs to absolute, normalize URL (lowercase hostname, remove trailing slash, remove fragment #), check if same hostname — if yes AND not visited AND not matching excludePatterns, add to queue
   - Create a minimal PageData object for now (url, title, links extracted, crawledAt timestamp). Leave forms/buttons/menus as empty arrays — the element mapper (Plan 02) will populate them.
   - Close the page after processing
   - Mark URL as visited
4. After 50 pages, log a warning: "Discovered 50+ pages. Crawl continuing..."
5. If maxPages > 0, stop after that many pages
6. Return CrawlResult with pages array, empty brokenLinks (link validator fills later), counts, and duration

**URL normalization:** Implement a simple `normalizeUrl(url: string): string` function:
- Remove fragment (#...)
- Remove trailing slash (except root /)
- Lowercase the hostname
- Sort query parameters alphabetically
- Remove default ports (80 for http, 443 for https)

**Exclude patterns:** Simple glob matching using string checks:
- `*.pdf` matches URLs ending in .pdf
- `*/admin/*` matches URLs with /admin/ in the path
- Use basic string matching (startsWith, endsWith, includes) — don't over-engineer regex

**Page processing:** Use Promise.allSettled for concurrent page processing within each batch. Catch and log errors per page without failing the entire crawl.

**Important:** After opening each page with BrowserManager.newPage(), the cookie banner is already dismissed (BrowserManager handles this). Wait briefly for content render, then extract links.

Create `src/discovery/index.ts` barrel export:
```typescript
export { SiteCrawler } from './crawler.js';
export type { CrawlerOptions } from './crawler.js';
```

Add header comment to crawler.ts explaining its purpose.
  </action>
  <verify>
Run `npx tsc --noEmit` — no type errors.

Verify the crawler module exists and exports correctly:
```bash
node -e "import('./dist/discovery/index.js').then(m => console.log(Object.keys(m)))"
```
(after `npm run build`)
  </verify>
  <done>SiteCrawler class crawls all same-hostname pages using BrowserManager, deduplicates URLs, respects concurrency limits, and returns CrawlResult with PageData array.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with zero errors
- `src/types/discovery.ts` exports all listed types
- `src/discovery/crawler.ts` exports SiteCrawler class
- SiteCrawler uses BrowserManager (not raw Playwright or Crawlee)
- URL normalization removes fragments, trailing slashes, and sorts query params
- Concurrency is limited to maxConcurrency (default 3)
</verification>

<success_criteria>
- All Phase 2 types defined and exported
- SiteCrawler recursively follows same-hostname links
- Visited URLs are deduplicated (no re-crawls)
- Concurrent page processing with configurable limit
- Warning emitted after 50 pages
- CrawlResult returned with all page data
</success_criteria>

<output>
After completion, create `.planning/phases/02-discovery-planning/02-01-SUMMARY.md`
</output>
