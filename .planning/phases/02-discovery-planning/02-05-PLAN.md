---
phase: 02-discovery-planning
plan: 05
type: execute
wave: 3
depends_on: ["02-01", "02-02", "02-03", "02-04"]
files_modified:
  - src/discovery/discovery-pipeline.ts
  - src/discovery/index.ts
  - src/index.ts
autonomous: true

must_haves:
  truths:
    - "User can run `npx afterburn <url>` and the tool discovers all pages, elements, and generates workflow plans"
    - "Crawl progress is shown in real-time (page count, current URL)"
    - "Sitemap tree is printed to console after crawl completes"
    - "Discovered workflow plans are printed for user review before Phase 3 would execute them"
    - "User can pass `--flows 'signup, checkout'` to prioritize specific workflows"
    - "Discovery artifact is saved as JSON for downstream pipeline consumption"
    - "Broken links are reported during crawl output"
  artifacts:
    - path: "src/discovery/discovery-pipeline.ts"
      provides: "Orchestrates full discovery: crawl + elements + SPA + links + sitemap + workflows"
      exports: ["runDiscovery"]
      min_lines: 100
    - path: "src/index.ts"
      provides: "Updated entry point with Phase 2 discovery pipeline"
      min_lines: 50
  key_links:
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/crawler.ts"
      via: "SiteCrawler.crawl() with pageProcessor callback and additionalUrls for SPA routes"
      pattern: "SiteCrawler|crawler\\.crawl|pageProcessor|additionalUrls"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/element-mapper.ts"
      via: "discoverElements() + discoverHiddenElements() called inside pageProcessor callback"
      pattern: "discoverElements|discoverHiddenElements"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/spa-detector.ts"
      via: "detectSPAFramework() + interceptRouteChanges() on first page, routes fed as additionalUrls"
      pattern: "detectSPAFramework|interceptRouteChanges|additionalUrls"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/link-validator.ts"
      via: "validateLinks() called inside pageProcessor callback"
      pattern: "validateLinks"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/sitemap-builder.ts"
      via: "buildSitemap() after crawl completes"
      pattern: "buildSitemap"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/planning/workflow-planner.ts"
      via: "WorkflowPlanner.generatePlans() with sitemap"
      pattern: "WorkflowPlanner|generatePlans"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/artifacts/artifact-storage.ts"
      via: "ArtifactStorage.save() for DiscoveryArtifact"
      pattern: "artifactStorage\\.save"
    - from: "src/index.ts"
      to: "src/discovery/discovery-pipeline.ts"
      via: "runDiscovery() called from main entry point"
      pattern: "runDiscovery"
---

<objective>
Wire all Phase 2 modules into a complete discovery pipeline and update the CLI entry point to run it end-to-end.

Purpose: This is the integration plan that connects crawler, element mapper, SPA detector, link validator, sitemap builder, and workflow planner into a single `runDiscovery()` function. It also updates the main entry point to use the discovery pipeline instead of the Phase 1 demo. This plan makes Phase 2 actually work.

Output: `src/discovery/discovery-pipeline.ts` orchestrating the full discovery flow, updated `src/index.ts` with Phase 2 pipeline.
</objective>

<execution_context>
@C:\Users\Shiv_\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Shiv_\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-discovery-planning/02-CONTEXT.md
@src/types/artifacts.ts
@src/browser/browser-manager.ts
@src/artifacts/artifact-storage.ts
@src/cli/progress.ts
@src/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build discovery pipeline orchestrator (crawl + elements + SPA + links + sitemap)</name>
  <files>src/discovery/discovery-pipeline.ts</files>
  <action>
Create `src/discovery/discovery-pipeline.ts` with a `runDiscovery()` function that orchestrates the entire Phase 2 flow.

**Imports:**
- `SiteCrawler` from `./crawler.js`
- `detectSPAFramework`, `interceptRouteChanges` from `./spa-detector.js`
- `discoverElements`, `discoverHiddenElements` from `./element-mapper.js`
- `validateLinks` from `./link-validator.js`
- `buildSitemap`, `printSitemapTree` from `./sitemap-builder.js`
- `WorkflowPlanner` from `../planning/index.js`
- `GeminiClient` from `../ai/index.js`
- `BrowserManager` from `../browser/index.js`
- `ScreenshotManager` from `../screenshots/index.js`
- `ArtifactStorage` from `../artifacts/index.js`
- Types from `../types/discovery.js`

**Interface:**
```typescript
interface DiscoveryOptions {
  targetUrl: string;
  sessionId: string;
  userHints?: string[];      // from --flows flag
  maxPages?: number;         // 0 = unlimited (default)
  onProgress?: (message: string) => void;
}

async function runDiscovery(options: DiscoveryOptions): Promise<DiscoveryArtifact>
```

**Orchestration flow (steps 1-6 — crawl and structure):**

1. **Initialize** BrowserManager, ScreenshotManager, ArtifactStorage.

2. **Launch browser** via BrowserManager.launch().

3. **SPA Detection (first page only):**
   - Open seed URL via BrowserManager.newPage()
   - Run `detectSPAFramework(page)` to identify framework
   - If SPA detected: run `interceptRouteChanges(page)` to discover client-side routes
   - Store discovered SPA routes for use as `additionalUrls`
   - Log: "Detected {framework} app" or "No SPA framework detected"
   - Close this initial page

4. **Define the pageProcessor callback** that will be passed to SiteCrawler. This callback is called for each crawled page BEFORE the page is closed (the crawler already supports this via `CrawlerOptions.pageProcessor` from Plan 01). The callback:
   - Runs `discoverElements(page, url)` to get forms, buttons, links, menus
   - Runs `discoverHiddenElements(page, url)` to trigger modals/dropdowns
   - Merges visible + hidden results
   - Captures a screenshot via ScreenshotManager.capture(page, `page-{index}`)
   - Runs `validateLinks(links, page)` to check for broken internal links
   - Returns `Partial<PageData>` with forms, buttons, menus, otherInteractive, screenshotRef, and collects brokenLinks in a closure variable
   - Reports progress: "Crawled: {url} ({count} pages found)"

   **NOTE:** The SiteCrawler (from Plan 01) already accepts `pageProcessor: (page: Page, url: string) => Promise<Partial<PageData>>` in CrawlerOptions. This task USES that API — it does NOT modify the crawler.

5. **Construct SiteCrawler** with BrowserManager and options:
   ```typescript
   const crawler = new SiteCrawler(browserManager, {
     maxPages: options.maxPages,
     onPageCrawled: (url, count) => options.onProgress?.(`Crawled: ${url} (${count} pages)`),
     pageProcessor: pageProcessorCallback,
     additionalUrls: spaRoutes,  // routes from step 3
   });
   ```
   The `additionalUrls` option (from Plan 01) seeds SPA-discovered routes into the crawl queue. This is how client-side routes get crawled alongside traditional link-discovered pages.

6. **Run crawler** with `crawler.crawl(seedUrl)`. Collect all PageData and accumulated BrokenLink results.

7. **Build sitemap** from collected PageData using `buildSitemap()`. Print tree to console using `printSitemapTree()`.

**Steps 8-10 (AI workflow + save) are handled in Task 2 of this plan.** This task creates the function shell, implements steps 1-7, and leaves placeholder comments for the workflow generation and artifact save steps that Task 2 will fill in.

**Error handling:** Wrap each major step in try-catch. If element discovery fails on a page, log warning and continue with partial data. Only crash on critical errors (browser won't launch, seed URL unreachable).

Export runDiscovery function. Add header comment.
  </action>
  <verify>Run `npx tsc --noEmit` — no type errors.</verify>
  <done>runDiscovery orchestrates crawl + per-page element discovery + SPA detection + link validation + sitemap building. Uses crawler's pageProcessor callback and additionalUrls API. Progress reported via callbacks.</done>
</task>

<task type="auto">
  <name>Task 2: Add AI workflow generation, artifact save, and update entry point</name>
  <files>src/discovery/discovery-pipeline.ts, src/discovery/index.ts, src/index.ts</files>
  <action>
**Complete runDiscovery() with workflow generation and artifact save (steps 8-10):**

8. **Generate workflow plans:**
   - Initialize GeminiClient and WorkflowPlanner
   - Call `WorkflowPlanner.generatePlans(sitemap, userHints)`
   - Handle case where GEMINI_API_KEY is not set: skip workflow generation, log warning "GEMINI_API_KEY not set — skipping AI workflow planning"
   - Print discovered workflows to console:
     ```
     Discovered Workflows:
       1. User Signup Flow (critical) - 5 steps
       2. Contact Form Submission (important) - 3 steps
       3. Navigation: Home > Dashboard (nice-to-have) - 2 steps
     ```

9. **Save discovery artifact** via ArtifactStorage.save() with stage 'discovery'.

10. **Cleanup:** Close browser via BrowserManager.close().

11. **Return** the DiscoveryArtifact.

**Error handling for workflow step:** If workflow planning fails (API error, parsing error), continue without workflow plans — log warning but don't fail the pipeline. Workflows are valuable but not critical for the discovery artifact.

**Update `src/discovery/index.ts`** to export all Phase 2 modules:
```typescript
export { SiteCrawler } from './crawler.js';
export type { CrawlerOptions } from './crawler.js';
export { detectSPAFramework, interceptRouteChanges } from './spa-detector.js';
export { discoverElements, discoverHiddenElements } from './element-mapper.js';
export { validateLinks } from './link-validator.js';
export { buildSitemap, printSitemapTree } from './sitemap-builder.js';
export { runDiscovery } from './discovery-pipeline.js';
```

**Update `src/index.ts`** to use the Phase 2 discovery pipeline:

The updated main function should:

1. Keep the existing Phase 1 setup: `ensureBrowserInstalled()`, URL parsing, session ID generation.

2. Parse `--flows` flag from command line:
   - Check for `--flows` in process.argv
   - If found, take the next argument as comma-separated flow hints
   - Split by comma, trim whitespace: `"signup, checkout"` -> `["signup", "checkout"]`
   - Example: `npx afterburn https://example.com --flows "signup, checkout"`

3. Run discovery pipeline:
   ```typescript
   const result = await runDiscovery({
     targetUrl,
     sessionId,
     userHints: flowHints,
     onProgress: (msg) => console.log(`  ${msg}`),
   });
   ```

4. Print summary after discovery completes:
   ```
   Discovery Complete!
     Pages found: {N}
     Interactive elements: {forms} forms, {buttons} buttons, {links} links
     Broken links: {N} found
     SPA framework: {framework or "None detected"}
     Workflow plans: {N} generated

   Site Map:
   {printSitemapTree output}

   Workflows:
     1. {name} ({priority}) - {steps} steps
     2. ...

   Artifact saved: .afterburn/artifacts/discovery-{sessionId}.json
   ```

5. Keep error handling and process.exit patterns from Phase 1.

**Important:** Use `withSpinner` from `src/cli/progress.ts` for long-running operations (browser launch, SPA detection, sitemap building, workflow generation). Use plain console.log for per-page crawl progress (a spinner hides individual page progress).

**Note on argument parsing:** Keep the simple process.argv approach (Commander.js deferred to Phase 6). Parse argv manually:
- `process.argv[2]` = target URL
- Check for `--flows` flag and its value
- This is intentionally simple and will be replaced by Commander.js later.
  </action>
  <verify>
Run `npx tsc --noEmit` — no type errors.

Run `npm run build` — compilation succeeds.

Verify the tool starts without crashing (won't complete without a real URL, but should at least get to "Usage: afterburn <url>" when run without args):
```bash
node dist/index.js
```
  </verify>
  <done>Entry point updated to run Phase 2 discovery pipeline. --flows flag supported for user workflow hints. Discovery results printed in plain English. Artifact saved for downstream consumption. Tool degrades gracefully without GEMINI_API_KEY.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with zero errors
- `npm run build` compiles successfully
- `node dist/index.js` shows usage message (no crash)
- All Phase 2 modules are properly exported from barrel files
- Discovery pipeline wires all modules: crawler, element mapper, SPA detector, link validator, sitemap builder, workflow planner
- Pipeline uses crawler's pageProcessor callback (does NOT modify crawler)
- Pipeline passes SPA routes as additionalUrls to crawler (does NOT modify crawler)
- --flows flag is parsed from command line arguments
- DiscoveryArtifact is saved to .afterburn/artifacts/
</verification>

<success_criteria>
- Running `npx afterburn <url>` triggers full discovery pipeline
- Progress is shown in real-time (page-by-page during crawl)
- Sitemap tree is printed after crawl
- Workflow plans are generated and displayed
- --flows hints are incorporated into workflow generation
- Broken links are reported
- Discovery artifact JSON is saved for Phase 3 consumption
- Tool degrades gracefully without GEMINI_API_KEY (skips workflow planning)
</success_criteria>

<output>
After completion, create `.planning/phases/02-discovery-planning/02-05-SUMMARY.md`
</output>
