---
phase: 02-discovery-planning
plan: 05
type: execute
wave: 3
depends_on: ["02-01", "02-02", "02-03", "02-04"]
files_modified:
  - src/discovery/discovery-pipeline.ts
  - src/discovery/index.ts
  - src/index.ts
autonomous: true

must_haves:
  truths:
    - "User can run `npx afterburn <url>` and the tool discovers all pages, elements, and generates workflow plans"
    - "Crawl progress is shown in real-time (page count, current URL)"
    - "Sitemap tree is printed to console after crawl completes"
    - "Discovered workflow plans are printed for user review before Phase 3 would execute them"
    - "User can pass `--flows 'signup, checkout'` to prioritize specific workflows"
    - "Discovery artifact is saved as JSON for downstream pipeline consumption"
    - "Broken links are reported during crawl output"
  artifacts:
    - path: "src/discovery/discovery-pipeline.ts"
      provides: "Orchestrates full discovery: crawl + elements + SPA + links + sitemap + workflows"
      exports: ["runDiscovery"]
      min_lines: 100
    - path: "src/index.ts"
      provides: "Updated entry point with Phase 2 discovery pipeline"
      min_lines: 50
  key_links:
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/crawler.ts"
      via: "SiteCrawler.crawl() for recursive page discovery"
      pattern: "SiteCrawler|crawler\\.crawl"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/element-mapper.ts"
      via: "discoverElements() + discoverHiddenElements() per page"
      pattern: "discoverElements|discoverHiddenElements"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/spa-detector.ts"
      via: "detectSPAFramework() + interceptRouteChanges() on first page"
      pattern: "detectSPAFramework|interceptRouteChanges"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/link-validator.ts"
      via: "validateLinks() per page"
      pattern: "validateLinks"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/discovery/sitemap-builder.ts"
      via: "buildSitemap() after crawl completes"
      pattern: "buildSitemap"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/planning/workflow-planner.ts"
      via: "WorkflowPlanner.generatePlans() with sitemap"
      pattern: "WorkflowPlanner|generatePlans"
    - from: "src/discovery/discovery-pipeline.ts"
      to: "src/artifacts/artifact-storage.ts"
      via: "ArtifactStorage.save() for DiscoveryArtifact"
      pattern: "artifactStorage\\.save"
    - from: "src/index.ts"
      to: "src/discovery/discovery-pipeline.ts"
      via: "runDiscovery() called from main entry point"
      pattern: "runDiscovery"
---

<objective>
Wire all Phase 2 modules into a complete discovery pipeline and update the CLI entry point to run it end-to-end.

Purpose: This is the integration plan that connects crawler, element mapper, SPA detector, link validator, sitemap builder, and workflow planner into a single `runDiscovery()` function. It also updates the main entry point to use the discovery pipeline instead of the Phase 1 demo. This plan makes Phase 2 actually work.

Output: `src/discovery/discovery-pipeline.ts` orchestrating the full discovery flow, updated `src/index.ts` with Phase 2 pipeline.
</objective>

<execution_context>
@C:\Users\Shiv_\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Shiv_\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-discovery-planning/02-CONTEXT.md
@src/types/artifacts.ts
@src/browser/browser-manager.ts
@src/artifacts/artifact-storage.ts
@src/cli/progress.ts
@src/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build discovery pipeline orchestrator</name>
  <files>src/discovery/discovery-pipeline.ts</files>
  <action>
Create `src/discovery/discovery-pipeline.ts` with a `runDiscovery()` function that orchestrates the entire Phase 2 flow.

**Imports:**
- `SiteCrawler` from `./crawler.js`
- `detectSPAFramework`, `interceptRouteChanges` from `./spa-detector.js`
- `discoverElements`, `discoverHiddenElements` from `./element-mapper.js`
- `validateLinks` from `./link-validator.js`
- `buildSitemap`, `printSitemapTree` from `./sitemap-builder.js`
- `WorkflowPlanner` from `../planning/index.js`
- `GeminiClient` from `../ai/index.js`
- `BrowserManager` from `../browser/index.js`
- `ScreenshotManager` from `../screenshots/index.js`
- `ArtifactStorage` from `../artifacts/index.js`
- Types from `../types/discovery.js`

**Interface:**
```typescript
interface DiscoveryOptions {
  targetUrl: string;
  sessionId: string;
  userHints?: string[];      // from --flows flag
  maxPages?: number;         // 0 = unlimited (default)
  onProgress?: (message: string) => void;
}

async function runDiscovery(options: DiscoveryOptions): Promise<DiscoveryArtifact>
```

**Orchestration flow:**

1. **Initialize** BrowserManager, ScreenshotManager, ArtifactStorage, and SiteCrawler.

2. **Launch browser** via BrowserManager.launch().

3. **SPA Detection (first page only):**
   - Open seed URL via BrowserManager.newPage()
   - Run `detectSPAFramework(page)` to identify framework
   - If SPA detected: run `interceptRouteChanges(page)` to discover client-side routes
   - Add any discovered SPA routes to the crawler queue
   - Log: "Detected {framework} app" or "No SPA framework detected"
   - Close this initial page

4. **Configure crawler** with an `onPageCrawled` callback that, for each page:
   - Runs `discoverElements(page, url)` to get forms, buttons, links, menus
   - Runs `discoverHiddenElements(page, url)` to trigger modals/dropdowns
   - Merges results into the page's PageData
   - Captures a screenshot via ScreenshotManager.capture(page, `page-{index}`)
   - Runs `validateLinks(links, page)` to check for broken internal links
   - Reports progress: "Crawled: {url} ({count} pages found)"

   **IMPORTANT:** The crawler (Plan 01) currently creates minimal PageData objects. Modify the crawler's crawl flow so it accepts an `onPageProcess` callback. The pipeline passes element discovery and link validation into this callback. The crawler calls it for each page before closing the page. This is cleaner than trying to process pages after the crawler closes them.

   Alternative approach if modifying crawler is complex: Have the crawler return pages WITHOUT closing them, let the pipeline process each page, then close. But this risks memory issues with many open pages. The callback approach is better.

   **Practical implementation:** The SiteCrawler should accept a `pageProcessor` option:
   ```typescript
   interface CrawlerOptions {
     // ... existing options
     pageProcessor?: (page: Page, url: string) => Promise<Partial<PageData>>;
   }
   ```
   The crawler calls pageProcessor for each page and merges the returned partial PageData with the base data (url, title, links). This keeps the crawler focused on crawling while the pipeline handles element discovery.

5. **Run crawler** with SiteCrawler.crawl(seedUrl). Collect all PageData and BrokenLink results.

6. **Build sitemap** from collected PageData using buildSitemap(). Print tree to console using printSitemapTree().

7. **Generate workflow plans:**
   - Initialize GeminiClient and WorkflowPlanner
   - Call WorkflowPlanner.generatePlans(sitemap, userHints)
   - Handle case where GEMINI_API_KEY is not set: skip workflow generation, log warning "GEMINI_API_KEY not set — skipping AI workflow planning"
   - Print discovered workflows to console:
     ```
     Discovered Workflows:
       1. User Signup Flow (critical) - 5 steps
       2. Contact Form Submission (important) - 3 steps
       3. Navigation: Home > Dashboard (nice-to-have) - 2 steps
     ```

8. **Save discovery artifact** via ArtifactStorage.save() with stage 'discovery'.

9. **Cleanup:** Close browser via BrowserManager.close().

10. **Return** the DiscoveryArtifact.

**Error handling:** Wrap each major step in try-catch. If element discovery fails on a page, log warning and continue with partial data. If workflow planning fails, continue without workflow plans. Only crash on critical errors (browser won't launch, seed URL unreachable).

Export runDiscovery function. Add header comment.
  </action>
  <verify>Run `npx tsc --noEmit` — no type errors.</verify>
  <done>runDiscovery orchestrates the full discovery pipeline: SPA detection, recursive crawl with per-page element discovery, link validation, sitemap construction, and AI workflow planning. Progress is reported via callbacks.</done>
</task>

<task type="auto">
  <name>Task 2: Update entry point and barrel exports</name>
  <files>src/discovery/index.ts, src/index.ts</files>
  <action>
**Update `src/discovery/index.ts`** to export all Phase 2 modules:
```typescript
export { SiteCrawler } from './crawler.js';
export type { CrawlerOptions } from './crawler.js';
export { detectSPAFramework, interceptRouteChanges } from './spa-detector.js';
export { discoverElements, discoverHiddenElements } from './element-mapper.js';
export { validateLinks } from './link-validator.js';
export { buildSitemap, printSitemapTree } from './sitemap-builder.js';
export { runDiscovery } from './discovery-pipeline.js';
```

**Update `src/index.ts`** to use the Phase 2 discovery pipeline:

The updated main function should:

1. Keep the existing Phase 1 setup: `ensureBrowserInstalled()`, URL parsing, session ID generation.

2. Parse `--flows` flag from command line:
   - Check for `--flows` in process.argv
   - If found, take the next argument as comma-separated flow hints
   - Split by comma, trim whitespace: `"signup, checkout"` -> `["signup", "checkout"]`
   - Example: `npx afterburn https://example.com --flows "signup, checkout"`

3. Run discovery pipeline:
   ```typescript
   const result = await runDiscovery({
     targetUrl,
     sessionId,
     userHints: flowHints,
     onProgress: (msg) => console.log(`  ${msg}`),
   });
   ```

4. Print summary after discovery completes:
   ```
   Discovery Complete!
     Pages found: {N}
     Interactive elements: {forms} forms, {buttons} buttons, {links} links
     Broken links: {N} found
     SPA framework: {framework or "None detected"}
     Workflow plans: {N} generated

   Site Map:
   {printSitemapTree output}

   Workflows:
     1. {name} ({priority}) - {steps} steps
     2. ...

   Artifact saved: .afterburn/artifacts/discovery-{sessionId}.json
   ```

5. Keep error handling and process.exit patterns from Phase 1.

**Important:** Use `withSpinner` from `src/cli/progress.ts` for long-running operations (browser launch, crawl). The crawl itself should show page-by-page progress (not a spinner — a spinner hides individual page progress). Use spinner for: browser launch, SPA detection, sitemap building, workflow generation. Use plain console.log for: per-page crawl progress.

**Note on argument parsing:** Keep the simple process.argv approach (Commander.js deferred to Phase 6). Parse argv manually:
- `process.argv[2]` = target URL
- Check for `--flows` flag and its value
- This is intentionally simple and will be replaced by Commander.js later.
  </action>
  <verify>
Run `npx tsc --noEmit` — no type errors.

Run `npm run build` — compilation succeeds.

Verify the tool starts without crashing (won't complete without a real URL, but should at least get to "Usage: afterburn <url>" when run without args):
```bash
node dist/index.js
```
  </verify>
  <done>Entry point updated to run Phase 2 discovery pipeline. --flows flag supported for user workflow hints. Discovery results printed in plain English. Artifact saved for downstream consumption.</done>
</task>

<task type="auto">
  <name>Task 3: Update crawler to accept page processor callback</name>
  <files>src/discovery/crawler.ts</files>
  <action>
Modify the SiteCrawler class (created in Plan 01) to accept a `pageProcessor` callback in CrawlerOptions.

**Add to CrawlerOptions:**
```typescript
pageProcessor?: (page: Page, url: string) => Promise<Partial<PageData>>;
```

**Modify the crawl loop:** After navigating to each page and extracting basic data (url, title, links), call `pageProcessor(page, url)` if provided. Merge the returned Partial&lt;PageData&gt; (forms, buttons, menus, otherInteractive, screenshotRef, etc.) into the PageData object for that page. Only THEN close the page.

**Flow per page:**
1. Navigate to URL via BrowserManager.newPage(url)
2. Extract title: `await page.title()`
3. Extract links: find all `<a>` elements, get hrefs
4. If pageProcessor exists: `const extra = await pageProcessor(page, url)`
5. Merge: `Object.assign(pageData, extra)`
6. Close page: `await page.close()`
7. Add discovered links to queue (same-hostname, not visited)

This separates crawling (URL management, dedup, concurrency) from per-page processing (element discovery, screenshots, link validation). The pipeline owns the processing logic; the crawler owns the crawling logic.

**Also update the index.ts barrel** if any new exports are added.
  </action>
  <verify>Run `npx tsc --noEmit` — no type errors.</verify>
  <done>Crawler accepts pageProcessor callback. Discovery pipeline can inject element mapping, screenshot capture, and link validation per page without the crawler needing to know about those modules.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes with zero errors
- `npm run build` compiles successfully
- `node dist/index.js` shows usage message (no crash)
- All Phase 2 modules are properly exported from barrel files
- Discovery pipeline wires all modules: crawler, element mapper, SPA detector, link validator, sitemap builder, workflow planner
- --flows flag is parsed from command line arguments
- DiscoveryArtifact is saved to .afterburn/artifacts/
</verification>

<success_criteria>
- Running `npx afterburn <url>` triggers full discovery pipeline
- Progress is shown in real-time (page-by-page during crawl)
- Sitemap tree is printed after crawl
- Workflow plans are generated and displayed
- --flows hints are incorporated into workflow generation
- Broken links are reported
- Discovery artifact JSON is saved for Phase 3 consumption
- Tool degrades gracefully without GEMINI_API_KEY (skips workflow planning)
</success_criteria>

<output>
After completion, create `.planning/phases/02-discovery-planning/02-05-SUMMARY.md`
</output>
